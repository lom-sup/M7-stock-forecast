{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col,coalesce, when, trim,to_date, lower, to_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import boto3\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9b3a19-e556-4197-a171-ee630545f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSession() -> SparkSession:\n",
    "    \"\"\"Spark 세션을 생성하고 반환하는 함수\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"S3-Spark-Integration\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# master 사용하면 오류... 왜??\n",
    "# def getSparkSession() -> SparkSession:\n",
    "#     \"\"\"Spark 세션을 생성하고 반환하는 함수\"\"\"\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .master(\"local[*]\")\\\n",
    "#         .appName(\"S3-Spark-Integration\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#         .getOrCreate()\n",
    "#     return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5265211-a10b-4d77-9c9c-b46e04bca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquet_files(s3_bucket: str, s3_prefix: str):\n",
    "    \"\"\"S3 특정 경로에서 .parquet 파일들의 전체 경로를 리스트로 반환\"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "\n",
    "    full_paths_list = []\n",
    "    if \"Contents\" in response:\n",
    "        for obj in response[\"Contents\"]:\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\".parquet\"):\n",
    "                full_path = f\"s3a://{s3_bucket}/{key}\"\n",
    "                full_paths_list.append(full_path)\n",
    "    else:\n",
    "        print(\"해당 경로에 파일이 없습니다.\")\n",
    "    return full_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0175bed-677c-473f-94b4-b0cb5e26c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스키마 정의\n",
    "schema = StructType([\n",
    "    StructField(\"stock\", StringType(), True),    # 주식이름\n",
    "    StructField(\"source\", StringType(), True),    # 언론사\n",
    "    StructField(\"pub_date\", StringType(), True),    # 날짜 및 시간\n",
    "    StructField(\"headline\", StringType(), True),  # 제목\n",
    "    StructField(\"content\", StringType(), True)    # 내용\n",
    "])\n",
    "\n",
    "# SparkException: Parquet column cannot be converted,Expected: date, Found: BINARY. 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d055f7-638c-4d34-a3e0-f97aaf895971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "읽을 파일 리스트:\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2020/nyt_articles_2020.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2021/nyt_articles_2021.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2022/nyt_articles_2022.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2023/nyt_articles_2023.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2024/nyt_articles_2024.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2025/nyt_articles_2025_01.parquet\n",
      "  - s3a://de5-finalproj-team5/raw_data/NYTD/2025/nyt_articles_2025_02.parquet\n",
      "+------+------+--------------------+--------------------+--------------------+\n",
      "| stock|source|            pub_date|            headline|             content|\n",
      "+------+------+--------------------+--------------------+--------------------+\n",
      "|Amazon|  NULL|2021-01-02T10:00:...|A Canadian ‘Buy L...|TORONTO — The sno...|\n",
      "|Amazon|  NULL|2021-01-04T11:00:...|Hundreds of Googl...|OAKLAND, Calif. —...|\n",
      "|Amazon|  NULL|2021-01-04T16:31:...|Home Solar Is Gro...|The home solar bu...|\n",
      "|Amazon|  NULL|2021-01-04T21:43:...|Amazon, Berkshire...|A joint venture f...|\n",
      "|Amazon|  NULL|2021-01-05T05:07:...|Catch up: Quibi s...|                NULL|\n",
      "+------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# S3 버킷 및 경로\n",
    "s3_bucket = \"de5-finalproj-team5\"\n",
    "s3_prefix = \"raw_data/NYTD/\"\n",
    "\n",
    "# S3에서 .parquet 파일 전체 경로 리스트 가져오기\n",
    "parquet_files = list_parquet_files(s3_bucket, s3_prefix)\n",
    "print(\"읽을 파일 리스트:\")\n",
    "for f in parquet_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# 파일 리스트가 있으면 Spark로 읽기\n",
    "# 재사용시에는 특정 파일 이름에 해당하는 파일만 가져올 것\n",
    "if parquet_files:\n",
    "    # df = spark.read.parquet(*parquet_files)\n",
    "    df = spark.read.schema(schema).parquet(*parquet_files)\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"읽을 Parquet 파일이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555d0efe-f82d-4af0-8679-ac4be84977fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df \\\n",
    "    .withColumn(\n",
    "        \"source\",\n",
    "        coalesce(df[\"source\"], lit(\"NYTD\"))) \\\n",
    "    .withColumn(\n",
    "        \"pub_date\",\n",
    "        to_date(\"pub_date\", \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5423dc-9744-4fcb-9619-06eb0ed3d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 하지 않을 예정이면 df를 그대로 사용\n",
    "# 전처리 수행하여 cleaned 사용\n",
    "# df_cleaned = df\n",
    "\n",
    "# 컬럼명을 변경하여 새로운 DataFrame 생성\n",
    "# stock 컬럼을 티커 심볼로 매핑하는 컬럼 생성\n",
    "df_transformed = df_cleaned.select(\n",
    "    # 매핑 처리: 조건에 따라 티커 심볼로 치환\n",
    "    when(col(\"stock\") == \"Apple\", lit(\"AAPL\"))\n",
    "    .when(col(\"stock\") == \"Amazon\", lit(\"AMZN\"))\n",
    "    .when(col(\"stock\") == \"Google\", lit(\"GOOGL\"))\n",
    "    .when(col(\"stock\") == \"Microsoft\", lit(\"MSFT\"))\n",
    "    .when(col(\"stock\") == \"Facebook\", lit(\"META\"))\n",
    "    .when(col(\"stock\") == \"Tesla\", lit(\"TSLA\"))\n",
    "    .when(col(\"stock\") == \"Netflix\", lit(\"NVDA\"))\n",
    "    .otherwise(col(\"stock\")).alias(\"symbol\"),\n",
    "    col(\"source\"),\n",
    "    col(\"pub_date\").alias(\"datetime\"),\n",
    "    col(\"headline\"),\n",
    "    col(\"content\").alias(\"summary\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23435431-3351-4d5f-a288-df26a4d0022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|symbol|source|datetime  |headline                                                                                  |summary                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------+------+----------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|AMZN  |NYTD  |2021-01-02|A Canadian ‘Buy Local’ Effort Fights Amazon on Its Own Turf                               |TORONTO — The snow was falling outside Ali Haberstroh’s apartment in late November when the idea came to her.                                                                                                                                                                                                                                                                       |\n",
      "|AMZN  |NYTD  |2021-01-04|Hundreds of Google Employees Unionize, Culminating Years of Activism                      |OAKLAND, Calif. — More than 400 Google engineers and other workers have formed a union, the group revealed on Monday, capping years of growing activism at one of the world’s largest companies and presenting a rare beachhead for labor organizers in staunchly anti-union Silicon Valley.                                                                                        |\n",
      "|AMZN  |NYTD  |2021-01-05|Home Solar Is Growing, but Big Installers Are Still Losing Money                          |The home solar business is growing fast as thousands of homeowners install panels on their roofs to save money. Yet the biggest companies that install and finance home solar systems are reporting hundreds of millions of dollars in losses.                                                                                                                                      |\n",
      "|AMZN  |NYTD  |2021-01-05|Amazon, Berkshire and JPMorgan Will End Joint Health Care Venture                         |A joint venture formed by Amazon, Berkshire Hathaway and JPMorgan Chase to explore new ways to deliver health care to their employees is disbanding, according to a short statement on its website. The company, Haven, will cease operations at the end of February — three years after its arrival sent shock waves through the health care industry.                             |\n",
      "|AMZN  |NYTD  |2021-01-05|Catch up: Quibi shops its videos to Roku, and a venture to disrupt health care shuts down.|NULL                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|AMZN  |NYTD  |2021-01-05|A Cure for Politicians’ Stock Trades                                                      |The recent controversy over stock trading by lawmakers like Senator Kelly Loeffler of Georgia, who avoided significant losses by selling some shares after key meetings early in the pandemic, revived concerns about how Congress polices its members’ portfolios. In his first column of the year, Andrew proposes a simple way to stop politicians’ trading in individual stocks.|\n",
      "|AMZN  |NYTD  |2021-01-05|Aiming to keep small businesses alive, a Canadian website is proudly Not Amazon.          |What began as a Google spreadsheet with more than 160 Canadian businesses collated from memory and research has become a directory of hundreds that have a website and a high-quality photo and offer nationwide shipping, curbside pickup or delivery.                                                                                                                             |\n",
      "|AMZN  |NYTD  |2021-01-06|V.R. Is Not a Hit. That’s OK.                                                             |This article is part of the On Tech newsletter. You can sign up here to receive it weekdays.                                                                                                                                                                                                                                                                                        |\n",
      "|AMZN  |NYTD  |2021-01-06|The Tech That Will Invade Our Lives in 2021                                               |This year, the technologies that we will most likely hear the most about won’t be fancy devices like smartphones or big-screen television sets. It will be the stuff we don’t usually see: workhorse software and internet products that are finding their moment now.                                                                                                              |\n",
      "|AMZN  |NYTD  |2021-01-07|A Lesson in Tech Survival                                                                 |This article is part of the On Tech newsletter. You can sign up here to receive it weekdays.                                                                                                                                                                                                                                                                                        |\n",
      "+------+------+----------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdb30a0-e78a-4dbc-8e1d-3fdd2850c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- source: string (nullable = false)\n",
      " |-- datetime: date (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인\n",
    "\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731208d-7fd5-42f1-b311-e113059a047a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 중복 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9737cea3-f594-44f9-981d-c414763f5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql 이용 등록\n",
    "\n",
    "df_transformed.createOrReplaceTempView(\"nyt_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50671909-1b37-49a3-b867-f92e213b5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.sql(\"\"\"\n",
    "    SELECT count(DISTINCT symbol)\n",
    "    FROM nyt_articles\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c118d07-1377-4dda-b30f-8c2e9b6b4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT symbol)|\n",
      "+----------------------+\n",
      "|                     7|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fe8ae87-fc63-4f30-a722-ba789a315651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+----------+------+---------------+\n",
      "|headline                                      |datetime  |symbol|duplicate_count|\n",
      "+----------------------------------------------+----------+------+---------------+\n",
      "|How ‘Save the Children’ Is Keeping QAnon Alive|2020-09-29|META  |2              |\n",
      "+----------------------------------------------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_duplicate_all = spark.sql(\"\"\"\n",
    "    SELECT headline, datetime, symbol, COUNT(*) AS duplicate_count\n",
    "    FROM nyt_articles\n",
    "    GROUP BY headline, datetime, symbol\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "df_duplicate_all.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344b42d-79a0-4259-af0f-4396983c7f41",
   "metadata": {},
   "source": [
    "### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78013875-0ec4-4fb8-a302-243c2a2329da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장 완료!\n",
      "파일명이 `analytic_data/NYTD/part-00000-dca80dcb-570c-496b-afa6-d8cfed0dde79-c000.snappy.parquet`에서 `analytic_data/NYTD/nytd_2020_202502_data.parquet`으로 변경되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 저장할 S3 경로 (폴더 경로)\n",
    "output_path = \"s3a://de5-finalproj-team5/analytic_data/NYTD/\"\n",
    "\n",
    "# 하나의 파일로 저장 (랜덤한 파일명으로 저장됨)\n",
    "df_transformed.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"데이터 저장 완료!\")\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"de5-finalproj-team5\"\n",
    "prefix = \"analytic_data/NYTD/\"\n",
    "\n",
    "# S3에 저장된 파일 목록 가져오기\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# 자동 생성된 Parquet 파일 찾기\n",
    "# bulk로 한번만 실행하는 logic이기에 가능. 다음번에 재사용 한다면 수정 필요 \n",
    "parquet_file = None\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    if obj[\"Key\"].endswith(\".parquet\"):\n",
    "        parquet_file = obj[\"Key\"]\n",
    "        break  # 첫 번째 parquet 파일을 찾으면 중단\n",
    "\n",
    "if parquet_file:\n",
    "    # 원하는 파일명으로 복사 (S3 내에서 이동)\n",
    "    new_filename = \"analytic_data/NYTD/nytd_2020_202502_data.parquet\"\n",
    "    copy_source = {'Bucket': bucket_name, 'Key': parquet_file}\n",
    "    \n",
    "    s3_client.copy_object(Bucket=bucket_name, CopySource=copy_source, Key=new_filename)\n",
    "\n",
    "    # 기존 자동 생성된 파일 삭제\n",
    "    s3_client.delete_object(Bucket=bucket_name, Key=parquet_file)\n",
    "\n",
    "    print(f\"파일명이 `{parquet_file}`에서 `{new_filename}`으로 변경되었습니다.\")\n",
    "else:\n",
    "    print(\"Parquet 파일을 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69777fc8-368d-460b-83d9-7593af4b4d15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Before code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ee73d-36aa-4bc9-8655-ca0a914805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getFileFromS3(spark: SparkSession, s3_bucket: str, s3_prefix: str):\n",
    "#     \"\"\"S3에서 Parquet 파일을 읽어와 DataFrame 반환\"\"\"\n",
    "#     \"\"\"특정 Bucket 아래에 있는 Parquet 파일 전부 로딩\"\"\"\n",
    "#     s3_path = f\"s3a://{s3_bucket}/{s3_prefix}\"  # S3 경로 설정\n",
    "#     print(f\"Loading data from: {s3_path}\")\n",
    "    \n",
    "#     try:\n",
    "#         df = spark.read.parquet(s3_path)  # Parquet 파일 읽기\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading from S3: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Spark 세션 생성\n",
    "# spark = getSparkSession()\n",
    "\n",
    "# # S3 버킷 및 경로 설정\n",
    "# s3_bucket = \"de5-finalproj-team5\"  # 버킷명\n",
    "# s3_prefix = \"raw_data/NYTD/2023/nyt_articles_2023.parquet\"  # S3 내부의 파일 경로\n",
    "\n",
    "# # S3에서 파일 로드\n",
    "# df = getFileFromS3(spark, s3_bucket, s3_prefix)\n",
    "\n",
    "# # 데이터 확인\n",
    "# if df is not None:\n",
    "#     df.show(5)\n",
    "# else:\n",
    "#     print(\"DataFrame is empty or could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d1e04-bedb-477d-a715-4d4e268e5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 정규 표현식으로 알파벳, 숫지와 공백만 남긴 후 앞 뒤 공백제거\n",
    "\n",
    "# df_cleaned = df \\\n",
    "#     .withColumn(\n",
    "#         \"headline\",\n",
    "#         trim(regexp_replace(col(\"headline\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "#     ) \\\n",
    "#     .withColumn(\n",
    "#         \"content\",\n",
    "#         trim(regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "#     ) \\\n",
    "#     .withColumn(\n",
    "#         \"pub_date\",\n",
    "#         to_timestamp(\"pub_date\", \"yyyy-MM-dd'T'HH:mm:ssZ\")\n",
    "#     )\n",
    "\n",
    "\n",
    "# # uncased bert는 소문자 처리하기\n",
    "# # lower(regexp_replace(col(\"headline\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "# # lower(regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1729d4-1e2f-4d85-b297-39bce61a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중복 그룹을 정의 (headline, datetime, symbol이 같은 경우 같은 그룹)\n",
    "# window_spec = Window.partitionBy(\"headline\", \"datetime\", \"symbol\").orderBy(\"datetime\")\n",
    "\n",
    "# # 각 그룹별로 row_number를 추가\n",
    "# df_with_row = df_transformed.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# # row_number가 1인 레코드만 유지 (즉, 그룹별 하나만 남기고 삭제)\n",
    "# df_deduplicated = df_with_row.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# # sql 이용 등록\n",
    "\n",
    "# df_deduplicated.createOrReplaceTempView(\"nyt_articles_cleaned\")\n",
    "\n",
    "# df_deduplicate_all = spark.sql(\"\"\"\n",
    "#     SELECT headline, datetime, symbol, COUNT(*) AS duplicate_count\n",
    "#     FROM nyt_articles_cleaned\n",
    "#     GROUP BY headline, datetime, symbol\n",
    "#     HAVING COUNT(*) > 1\n",
    "# \"\"\")\n",
    "# df_duplicate_all.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d65f21f-3847-45d4-8fd4-b1dce8cf47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# # 저장할 S3 경로 설정\n",
    "# output_path = \"s3a://de5-finalproj-team5/analytic_data/NYTD/nytd_2020_202502_data.parquet\"\n",
    "\n",
    "# # # Parquet 파일로 저장 (overwrite 모드: 기존 파일 덮어쓰기)\n",
    "# # df_transformed.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# df_transformed.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# print(\"데이터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d97d926c-cdb6-40ef-af5f-64fd88fa6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 S3 저장된 파일 목록:\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/_SUCCESS\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00000-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00001-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00002-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00003-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00004-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00005-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n",
      "  - analytic_data/NYTD/nytd_2020_202502_data.parquet/part-00006-91ac7625-9827-43ef-99b0-38236d465c41-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# # 저장된 데이터 확인\n",
    "\n",
    "# s3_client = boto3.client(\"s3\")\n",
    "# bucket_name = \"de5-finalproj-team5\"\n",
    "# prefix = \"analytic_data/NYTD/\"\n",
    "\n",
    "# response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# print(\"S3 저장된 파일 목록:\")\n",
    "# for obj in response.get(\"Contents\", []):\n",
    "#     print(f\"  - {obj['Key']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
