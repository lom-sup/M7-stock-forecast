import sys
import boto3
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import regexp_replace, regexp_extract, col, coalesce, lit, udf, to_date, when, from_unixtime, date_format, length, year, month, collect_list, struct
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from datetime import datetime, timedelta
# import datetime
import re
from functools import reduce
from pyspark.sql.types import StringType



# Glue Job ì‹¤í–‰ì„ ìœ„í•œ ì¸ì ì²˜ë¦¬
args = getResolvedOptions(sys.argv, ["task_type", "is_incremental", "data_interval_start", "data_interval_end", "JOB_NAME"])

task_types = args["task_type"].split(",")
is_incremental = args['is_incremental'].lower() == 'true'

print(f"ğŸ” [INFO] ì²˜ë¦¬í•  task_type ëª©ë¡: {task_types}")
print(f"ğŸ” [INFO] is_incremental: {is_incremental}")

data_interval_start = datetime.fromisoformat(args["data_interval_start"])
data_interval_end = datetime.fromisoformat(args["data_interval_end"])

TARGET_DATE = data_interval_start.strftime("%Y_%m_%d")  # YYYY_MM_DD í˜•ì‹


spark = SparkSession.builder.appName(args["JOB_NAME"]).getOrCreate()
glueContext = GlueContext(spark.sparkContext)

# Job ê°ì²´ ì´ˆê¸°í™”
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

S3_BUCKET = "de5-finalproj-team5"

# --------------------------------------------------------------------------------
# S3 ë°ì´í„° ì½ê¸° ë° ì €ì¥
# --------------------------------------------------------------------------------
def get_file_from_S3(s3_file_path: str) -> DataFrame:
    try:
        df = spark.read.parquet(s3_file_path)
        if df is None or df.rdd.isEmpty():
            raise FileNotFoundError(f"{s3_file_path} ê²½ë¡œì˜ íŒŒì¼ì´ ë¹„ì–´ ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return df
    except AnalysisException as e:
        raise FileNotFoundError(f"{s3_file_path} ê²½ë¡œì˜ íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        raise RuntimeError(f"{s3_file_path} ê²½ë¡œì˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")


def save_file_to_S3_staging(df: DataFrame, target_file_path: str):
    try:
        df.write.mode("overwrite").parquet(target_file_path)
        return target_file_path
    except Exception as e:
        error_msg = f"{target_file_path} ê²½ë¡œì— íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}"
        print(error_msg)
        return error_msg

# --------------------------------------------------------------------------------
# ì „ì²˜ë¦¬ UDF ìƒì„±: content í´ë Œì§•
# --------------------------------------------------------------------------------
def preprocess_content(content):
    """
    ì½˜í…ì¸  ì „ì²˜ë¦¬:
        1. ì‹œì‘ê³¼ ëì˜ í°ë”°ì˜´í‘œ ì œê±°
        2. ì¤„ë°”ê¿ˆ(\n) ê¸°ì¤€ ë¶„í•  í›„, ê° ë¬¸ì¥ì´ íƒ­ 3ë²ˆ í˜¹ì€ ìŠ¤í˜ì´ìŠ¤ 12ë²ˆìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë§Œ ë‚¨ê¸°ê³ ,
            ë§Œì•½ "Related article"ì´ë©´ ê·¸ ë¬¸ì¥ê³¼ ë°”ë¡œ ë’¤ ë¬¸ì¥ì€ ê±´ë„ˆëœ€.
        3. ë‚¨ì€ ë¬¸ì¥ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°.
        4. ì½˜í…ì¸  ì‹œì‘ ë¶€ë¶„ì˜ "CNN", "CNN Business" ë“± ì ‘ë‘ì‚¬ë¥¼ ì œê±°.
    """
    if not content:
        return content

    content = content.strip('â€œâ€"')
    lines = content.splitlines()
    processed_lines = []
    skip_next = False
    for line in lines:
        if skip_next:
            skip_next = False
            continue
        if line.strip() == "Related article":
            skip_next = True
            continue
        if not re.match(r'^(?:\t{3}| {12})', line):
            continue
        processed_lines.append(line.strip())
    new_content = " ".join(processed_lines)
    new_content = re.sub(r'^(?:[A-Za-z\s]+)?\bCNN(?: Business)?\s*-\s*', '', new_content)
    return new_content

preprocess_content_udf = udf(preprocess_content, StringType())


# í‚¤ì›Œë“œ í´ë Œì§•
def map_keyword(keyword):
    if not keyword:
        return keyword
    keyword_mapping = {
    "apple": "AAPL",
    "google": "GOOGL",
    "alphabet": "GOOGL",
    "facebook": "META",
    "meta": "META",
    "microsoft": "MSFT",
    "amazon": "AMZN",
    "tesla": "TSLA",
    "elon musk": "TSLA",
    "nvidia": "NVDA"
    }
    return keyword_mapping.get(keyword.lower(), keyword.upper())

map_keyword_udf = udf(map_keyword, StringType())

def get_all_parquet_files(s3_client, bucket, prefix):
    """S3ì—ì„œ íŠ¹ì • prefix ì•„ë˜ ëª¨ë“  .parquet íŒŒì¼ ê°€ì ¸ì˜¤ê¸°"""
    files = []
    continuation_token = None

    while True:
        response = (
            s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)
            if continuation_token
            else s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
        )
        if "Contents" in response:
            files.extend(
                [f"s3a://{bucket}/{obj['Key']}" 
                for obj in response.get("Contents", []) 
                if obj["Key"].endswith(".parquet")]
            )

        continuation_token = response.get("NextContinuationToken")
        if not continuation_token:
            break

    return files

def list_parquet_files(s3_client, bucket, is_incremental, target_date=None):
    """
    - Full load: ì „ì²´ ê²½ë¡œ ì¤‘, 2020~2024 í´ë”ëŠ” ê·¸ëŒ€ë¡œ í¬í•¨í•˜ê³ ,
    2025 í´ë”ì—ì„œëŠ” íŒŒì¼ëª…ì— '2025_01' ë˜ëŠ” '2025_02'ê°€ í¬í•¨ëœ íŒŒì¼ë§Œ ì„ íƒ.
    - Incremental load: "raw_data/NYTD/2025/incremental/" ê²½ë¡œì—ì„œ target_dateê°€ í‚¤ì— í¬í•¨ëœ íŒŒì¼ë§Œ ì„ íƒ. íŒŒì¼ ëª©ë¡ ì „ë¶€ë¥¼ ê°€ì ¸ì™€ filterí•˜ë‹ˆ ì˜¤ë¥˜ ë°œìƒ.
    """
    
    print(f"ğŸ” [INFO] list_parquet_files() ì‹¤í–‰ë¨ - is_incremental: {is_incremental}, target_date: {TARGET_DATE}")
    
    if is_incremental:
        prefix = "raw_data/NYTD/2025/incremental/"
        all_files = get_all_parquet_files(s3_client, bucket, prefix)
        
        print(f"ğŸ“‚ [DEBUG] {prefix} ê²½ë¡œì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ íŒŒì¼ ëª©ë¡: {all_files}")
        
        if not all_files:
            print(f"âŒ [ERROR] {prefix} ê²½ë¡œì—ì„œ ì•„ë¬´ íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŒ!")
            return []
        
        # if not target_date or target_date.lower() == "none":
        #     target_date = (data_interval_start - timedelta(days=1)).strftime("%Y_%m_%d")  # YYYY_MM_DD í˜•ì‹
            
        filtered_files = [f for f in all_files if f.split('/')[-1].startswith(f"nyt_articles_{TARGET_DATE}")]
            
        print(f"ğŸ“‚ [DEBUG] í•„í„°ë§ëœ íŒŒì¼ ëª©ë¡: {filtered_files}")
            
        if not filtered_files:
            print(f"There is no {TARGET_DATE} NYTD Data.")
            return []
            
        return filtered_files

    else: # fullì¸ ê²½ìš°, ë”± í•œë²ˆë§Œ ì‹¤í–‰
        prefix = "raw_data/NYTD/"
        all_files = get_all_parquet_files(s3_client, bucket, prefix)
        
        print(f"ğŸ“‚ [DEBUG] {prefix} ê²½ë¡œì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ íŒŒì¼ ëª©ë¡: {all_files}")
        
        if not all_files:
            print(f"âŒ [ERROR] {prefix} ê²½ë¡œì—ì„œ ì•„ë¬´ íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŒ!")
            return []
        
        filtered_files = []
        base_path = f"s3a://{bucket}/raw_data/NYTD/"
        
        for f in all_files:
            subpath = f.replace(base_path, "")
            
            # 2020~2024 í´ë”ì˜ ê²½ìš°
            if subpath.startswith(("2020/", "2021/", "2022/", "2023/", "2024/")):
                filtered_files.append(f)
                
            # 2025 í´ë”ì˜ ê²½ìš°, íŒŒì¼ëª…ì— '2025_01' ë˜ëŠ” '2025_02'ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨.
            elif subpath.startswith("2025/"):
                filename = subpath.split("/")[-1]
                if "2025_01" in filename or "2025_02" in filename:
                    filtered_files.append(f)
                    
        print(f"ğŸ“‚ [DEBUG] í•„í„°ë§ëœ íŒŒì¼ ëª©ë¡: {filtered_files}")    
        
        if not filtered_files:
            print(f"âŒ [ERROR] Full Loadì—ì„œë„ ì‚¬ìš©í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return []     
                    
        return filtered_files

def load_and_process_data(s3_client, is_incremental, target_date=None):
    """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬"""
    print(f"ğŸš€ [INFO] load_and_process_data ì‹¤í–‰ë¨ - is_incremental: {is_incremental}, target_date: {TARGET_DATE}")
    
    parquet_files = list_parquet_files(s3_client, S3_BUCKET, is_incremental, TARGET_DATE)

    if not parquet_files:
        raise ValueError("[ERROR] No NYTD parquet files to read.")
    
    # ì–´ì°¨í”¼ ì½ì„ parquetëŠ” ë§ì•„ë´¤ì 8ê°œ ì•„ë˜, ê·¸ëƒ¥ ì´ë¦„ ë‹¤ ë³´ì—¬ì£¼ê¸°
    print(f"ğŸ” Found parquet files: {parquet_files}")

    df = spark.read.parquet(*parquet_files)
    df_cleaned = df.withColumn("source", lit("NYTD")).withColumn("pub_date", to_date("pub_date", "yyyy-MM-dd'T'HH:mm:ssZ"))

    df_transformed = df_cleaned.select(
        when(col("stock") == "Apple", lit("AAPL"))
        .when(col("stock") == "Amazon", lit("AMZN"))
        .when(col("stock") == "Google", lit("GOOGL"))
        .when(col("stock") == "Microsoft", lit("MSFT"))
        .when(col("stock") == "Facebook", lit("META"))
        .when(col("stock") == "Tesla", lit("TSLA"))
        .when(col("stock") == "Netflix", lit("NVDA"))
        .otherwise(col("stock")).alias("symbol"),
        col("source"),
        col("pub_date").alias("datetime"),
        col("headline"),
        col("content").alias("summary")
    )
    df_transformed = df_transformed.dropDuplicates(["symbol", "datetime", "headline"])
    return df_transformed

# def delete_existing_parquet_files(s3_client, bucket, prefix):
#     """S3 í´ë” ë‚´ ê¸°ì¡´ Parquet íŒŒì¼ ì‚­ì œ, overwrittenë¡œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ë¶ˆê°€í•œ ê²½ìš° ëŒ€ë¹„"""
#     response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
#     if "Contents" in response:
#         for obj in response["Contents"]:
#             if obj["Key"].endswith(".parquet"):
#                 s3_client.delete_object(Bucket=bucket, Key=obj["Key"])
#                 print(f"[INFO] Deleted existing file: {obj['Key']}")

def save_data(df_transformed, s3_client, bucket, is_incremental, target_date, data_interval_start, data_interval_end):
    """ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    # full_output_path = f"s3://{bucket}/staging_data/news/full/"
    # # target_dateëŠ” "YYYY_MM_DD" í˜•ì‹ìœ¼ë¡œ ì „ë‹¬ëœë‹¤ê³  ê°€ì •
    # dt = datetime.datetime.strptime(target_date, "%Y_%m_%d")
    # ymd = dt.strftime("%Y%m%d")
    # incremental_output_path = f"s3://{bucket}/staging_data/news/incremental/NYTD/{ymd}/"
    
    """ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    full_output_path = f"s3://{bucket}/staging_data/news/full/"
    # DAG ì—ì„œ target_dateê°€ Noneê°’ìœ¼ë¡œ ë„˜ì–´ì™€ì„œ data_interval_endê°’ìœ¼ë¡œ ì§€ì •
    ymd = target_date.replace("_", "")
    incremental_output_path = f"s3://{bucket}/staging_data/news/incremental/NYTD/{ymd}/"

    if is_incremental:
        # `_SUCCESS`, `_$folder$` íŒŒì¼ ìƒì„±ì„ ë°©ì§€
        spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")
        
        # ê¸°ì¡´ S3 í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì‚­ì œ
        print(f"[DELETE] ê¸°ì¡´ S3 ë°ì´í„° ì‚­ì œ ì¤‘... ê²½ë¡œ: {incremental_output_path}")
        prefix = incremental_output_path.replace(f"s3://{bucket}/", "")
        
        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
        if "Contents" in response:
            for obj in response["Contents"]:
                s3_client.delete_object(Bucket=bucket, Key=obj["Key"])
                
        # delete_existing_parquet_files(s3_client, bucket, prefix)
        df_transformed.repartition(1).write.mode("overwrite").parquet(incremental_output_path)
        print("[INFO] Incremental Load ì™„ë£Œ")
    else:
        df_transformed.repartition(1).write.mode("overwrite").parquet(full_output_path)
        print("[INFO] Full Load ì™„ë£Œ: nytd.parquet")

# --------------------------------------------------------------------------------
# DataFrame ì „ì²˜ë¦¬ í•¨ìˆ˜. ì»¬ëŸ¼ëª… ì¬ì •ì˜ ë° í´ë Œì§• UDF ì ìš©.
# --------------------------------------------------------------------------------
def process_df(df: DataFrame) -> DataFrame:
    return df.withColumn("symbol", map_keyword_udf(col("keyword"))) \
            .withColumn("source", lit("CNN")) \
            .withColumn("datetime", date_format(col("date"), "yyyy-MM-dd")) \
            .withColumn("headline", col("title")) \
            .withColumn("summary", preprocess_content_udf(col("content")))
            

    
# --------------------------------------------------------------------------------
# staging_data -> analytics_data
# --------------------------------------------------------------------------------
def list_s3_parquet_files(bucket, prefix):
    """
    S3 ë²„í‚· ë‚´ íŠ¹ì • ê²½ë¡œì˜ ëª¨ë“  Parquet íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜
    """
    s3 = boto3.client("s3")
    objects = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)

    parquet_files = [
        f"s3a://{bucket}/{obj['Key']}" 
        for obj in objects.get("Contents", []) 
        if obj["Key"].endswith(".parquet")
    ]
    return parquet_files

def get_file_from_staging(spark: SparkSession, s3_bucket: str, base_s3_prefix: str, is_incremental: bool):
    """
    S3ì— ì €ì¥ëœ full ë˜ëŠ” incremental ë°ì´í„°ë¥¼ ì½ì–´ì™€ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë°˜í™˜
    """
    try:
        if not is_incremental:
            df = spark.read.parquet(f"s3a://{s3_bucket}/{base_s3_prefix}/full")
        else:
            dfs = []
            for source in sources:
                prefix = f"{base_s3_prefix}/incremental/{source}/"
                parquet_files = list_s3_parquet_files(s3_bucket, prefix)
                
                if parquet_files:
                    print(f"[INFO] {source}ì—ì„œ {len(parquet_files)}ê°œì˜ Parquet íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    dfs.append(spark.read.parquet(*parquet_files))
                else:
                    print(f"[WARN] {source} ë°ì´í„° ì—†ìŒ: {prefix}")
            
            if dfs:
                df = reduce(lambda a, b: a.union(b), dfs)
            else:
                print("[WARN] ì½ì–´ì˜¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                sys.exit(1)
                
        return df.coalesce(1) if df else None
    
    except Exception as e:
        print(f"[ERROR] get_file_from_staging ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

def save_to_analytics(df: DataFrame, s3_bucket: str, output_s3_prefix: str):
    try:
        # output_s3_prefix = "analytic_data/news"
        output_path = f"s3a://{s3_bucket}/{output_s3_prefix}"
        file_count = 0
        saved_paths = []

        print(f"[DEBUG] ì €ì¥ ê²½ë¡œ: {output_path}")
        print(f"[DEBUG] ë°ì´í„°í”„ë ˆì„ ì´ í–‰ ìˆ˜: {df.count()}")
        print(f"[DEBUG] ë°ì´í„°í”„ë ˆì„ ìŠ¤í‚¤ë§ˆ:")
        df.printSchema()
        
        # datetimeì„ ê°•ì œë¡œ stringìœ¼ë¡œ ë³€í™˜ (íƒ€ì… ë¶ˆì¼ì¹˜ í•´ê²°)
        df = df.withColumn("datetime", col("datetime").cast("string"))
        
        # NULL ê°’ í™•ì¸
        df_null_check = df.filter(col("datetime").isNull())
        print(f"[DEBUG] datetime NULLì¸ ë°ì´í„° ê°œìˆ˜: {df_null_check.count()}")
        df_null_check.show(5)
        
        # datetime ì»¬ëŸ¼ì´ 'YYYY-MM-DD' ë˜ëŠ” 'YYYY-MM-DDTHH:mm:ss.SSS' í˜•íƒœì¼ ê²½ìš°, 'YYYY-MM-DD'ë¡œ ë³€í™˜
        df = df.withColumn(
            "record_date",
            when(length(col("datetime")) > 10, to_date(col("datetime"), "yyyy-MM-dd'T'HH:mm:ss.SSS"))
            .when(regexp_extract(col("datetime"), "^[0-9]+$", 0) != "", to_date(from_unixtime(col("datetime").cast("bigint"), "yyyy-MM-dd")))
            .otherwise(to_date(col("datetime"), "yyyy-MM-dd"))
        )

        # ì—°ë„ì™€ ì›” ì»¬ëŸ¼ ì¶”ê°€
        df = df.withColumn("year", year(col("record_date"))).withColumn("month", month(col("record_date")))

        # ê³ ìœ í•œ ì—°-ì›” ì¡°í•©ì„ ê·¸ë£¹í™”í•˜ì—¬ ì €ì¥
        for row in df.select("year", "month").distinct().collect():
            yyyy, mm = row["year"], row["month"]
            partition_df = df.filter((col("year") == yyyy) & (col("month") == mm))
            partition_output = f"{output_path}/year={yyyy}/month={mm}"
            
            partition_count = partition_df.count()
            print(f"[DEBUG] ì €ì¥ ì¤‘: {partition_output}, ë°ì´í„° ê°œìˆ˜: {partition_count}")
            
            if partition_count == 0:
                print(f"[WARN] ì›” {mm}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            partition_df.select("symbol", "source", "datetime", "headline", "summary") \
                .write.mode("overwrite").parquet(partition_output)
            
            saved_paths.append(partition_output)
            file_count += 1

        print(f"[INFO] ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì´ {file_count}ê°œì˜ íŒŒì¼ì´ íŒŒí‹°ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return df.select("year", "month").distinct().collect(), saved_paths, file_count
    except Exception as e:
        print(f"[ERROR] save_to_analytics ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if "cnn_cleansing" in task_types:
    
    data_interval_start = datetime.fromisoformat(args["data_interval_start"])
    data_interval_end = datetime.fromisoformat(args["data_interval_end"])
    is_incremental = args['is_incremental'].lower() == 'true'
    
    
    print("cnn_cleansing ê°œë°œ ì¤‘...")
    """
        - í˜„ì¬ ì—°ë„ì™€ INCREMENTAL_DATEì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë§Œ ì½ì–´ ì „ì²˜ë¦¬ í›„ ì €ì¥
    """
    # ë‚ ì§œ
    current_year = data_interval_start.strftime("%Y")
    incremental_date_str = data_interval_start.strftime("%Y%m%d")
    # íŒŒì¼ ê²½ë¡œ
    base_file_path = "s3a://de5-finalproj-team5/raw_data/CNN"
    output_file_path = "s3a://de5-finalproj-team5/staging_data/news"
    file_path = f"{base_file_path}/{current_year}/cnn_article_{incremental_date_str}.parquet"
    
    try:
        df_daily = get_file_from_S3(file_path)
        df_daily_processed = process_df(df_daily)
        df_daily_final = df_daily_processed.select("symbol", "source", "datetime", "headline", "summary")
        output_daily_path = f"{output_file_path}/incremental/CNN/{incremental_date_str}"
        save_file_to_S3_staging(df_daily_final, output_daily_path)
        print("Incremental íŒŒì¼ ì „ì²˜ë¦¬ í›„ ì €ì¥ ì™„ë£Œ:", output_daily_path)
    except Exception as e:
        print(f"Incremental íŒŒì¼ ì½ê¸° ì—ëŸ¬(ë‚ ì§œ {incremental_date_str}): {e}")
        
if "finnhub_cleansing" in task_types:

    print(f"Incremental ì‹¤í–‰ ì‹œì‘... finnhub_cleansing")
    # pass

    # ê³¼ê±° ë°ì´í„°ë„ ì²˜ë¦¬í•˜ê¸° ìœ„í•˜ì—¬ datetime.today()ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ê°’ ì‚¬ìš©,
    # execution_date, data_interval_start, data_interval_end
    # data_interval_startì€ ì‹¤í–‰ë‚ ì§œê°€ ì•„ë‹ˆë¼ ìŠ¤ì¼€ì¤„ë§ì— ë”°ë¥¸ ì‹¤í–‰ë  ë‚ ì§œ!
    # ì˜ˆ) ìŠ¤ì¼€ì¤„ë§: ë§¤ì¼ ìƒˆë²½ 3ì‹œ
    # 3/15 ìˆ˜ë™ ì‹¤í–‰ì‹œ data_interval_startëŠ” 3/14 03:00 data_interval_endëŠ” 3/15 03:00
    
    
    # --------------------------------
    day_exp = data_interval_start.strftime("%Y%m%d") 

    # ğŸ”¹ S3 ê²½ë¡œ ì„¤ì •
    RAW_DATA_2025 = f"s3://{S3_BUCKET}/raw_data/FINNHUB/2025/incremental/finnhub_m7_news_{day_exp}.parquet"
    OUTPUT_PATH = f"s3://{S3_BUCKET}/staging_data/news/incremental/FINNHUB/{day_exp}"
    
    try:
        # `_SUCCESS`, `_$folder$` íŒŒì¼ ìƒì„±ì„ ë°©ì§€
        spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")
        
        # ê¸°ì¡´ S3 í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì‚­ì œ
        print(f"[DELETE] ê¸°ì¡´ S3 ë°ì´í„° ì‚­ì œ ì¤‘... ê²½ë¡œ: {OUTPUT_PATH}")
        s3 = boto3.client("s3")
        bucket = S3_BUCKET
        prefix = OUTPUT_PATH.replace(f"s3://{bucket}/", "")

        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
        if "Contents" in response:
            for obj in response["Contents"]:
                s3.delete_object(Bucket=bucket, Key=obj["Key"])

        print(f"[DELETE] ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")
        
        # ğŸ”¹ S3ì—ì„œ ë°ì´í„° ë¡œë“œ
        print(f"[LOAD] S3ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘... ê²½ë¡œ: {RAW_DATA_2025}")
        df = spark.read.parquet(RAW_DATA_2025)

        # ğŸ”¹ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°
        print(f"[CLEAN] ì¤‘ë³µ ì œê±° ì¤‘...")
        df = df.dropDuplicates(["symbol", "datetime", "headline"])
        
        # ğŸ”¹ datetime ì»¬ëŸ¼ì„ UTC ì‹œê°„ì„ ì œì™¸í•œ ë‚ ì§œ ê°’ìœ¼ë¡œ ë³€í™˜
        df = df.withColumn("datetime", from_unixtime(col("datetime"), "yyyy-MM-dd"))

        # ğŸ”¹ ë°ì´í„° ì „ì²˜ë¦¬ (URL, HTML, ì¶”ê°€ í…ìŠ¤íŠ¸ ì œê±°)
        print(f"[CLEAN] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        url_pattern = r"http[s]?://\S+"
        html_pattern = r"<.*?>"
        extra_text_pattern = r"\[.*?\]|\(.*?\)"
        whitespace_pattern = r"\s+"

        df_cleaned = df.withColumn("headline", regexp_replace(col("headline"), url_pattern, "")) \
                    .withColumn("headline", regexp_replace(col("headline"), html_pattern, "")) \
                    .withColumn("headline", regexp_replace(col("headline"), extra_text_pattern, "")) \
                    .withColumn("headline", regexp_replace(col("headline"), whitespace_pattern, " ")) \
                    .withColumn("summary", regexp_replace(col("summary"), url_pattern, "")) \
                    .withColumn("summary", regexp_replace(col("summary"), html_pattern, "")) \
                    .withColumn("summary", regexp_replace(col("summary"), extra_text_pattern, "")) \
                    .withColumn("summary", regexp_replace(col("summary"), whitespace_pattern, " "))

        # Parquet íŒŒì¼ì„ 1ê°œë§Œ ìœ ì§€í•˜ë„ë¡ ì„¤ì •
        df_single = df_cleaned.repartition(1)

        # S3ì— ì €ì¥ (Parquet í˜•ì‹, ë®ì–´ì“°ê¸° ì ìš©)
        print(f"[SAVE] ë°ì´í„° S3 ì €ì¥ ì¤‘... ê²½ë¡œ: {OUTPUT_PATH}")
        df_single.write.mode("overwrite").parquet(OUTPUT_PATH)

        print(f"[SUCCESS] {day_exp} ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {OUTPUT_PATH}")

    except Exception as e:
        print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ! Task Type: finnhub_cleansing, ì˜¤ë¥˜ ë‚´ìš©: {e}")
        sys.exit(1)  # ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ ì‹œ Glue Job ì¢…ë£Œ
    # --------------------------------
    
if "nytd_cleansing" in task_types:
    
    # NYT ë°ì´í„° í´ë Œì§• ë¡œì§ ì¶”ê°€
    # pass

    # --------------------------------
    """SparkSessionì„ í™œìš©í•˜ì—¬ ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰"""
    s3_client = boto3.client("s3")
    
    df_transformed = load_and_process_data(s3_client, is_incremental)
    
    print(f"ğŸ” [INFO] TARGET_DATE: {TARGET_DATE}")
    
    save_data(df_transformed, s3_client, S3_BUCKET, is_incremental, TARGET_DATE, data_interval_start, data_interval_end)  

    # --------------------------------
    
    
if "merge_news_data" in task_types:
    try:
        s3_bucket = "de5-finalproj-team5"
        base_s3_prefix = "staging_data/news"
        output_s3_prefix = "analytic_data/news"

        sources = ["FINNHUB", "CNN", "NYTD"]
        df = get_file_from_staging(spark, s3_bucket, base_s3_prefix, is_incremental)
        if df is None:
            print("[WARN] S3ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        distinct_dates, saved_paths, file_count = save_to_analytics(df, s3_bucket, output_s3_prefix)

        for saved_date in distinct_dates:
            print(f"[INFO] ì €ì¥ëœ ê¸°ì‚¬ ë‚ ì§œ: {saved_date}")
        for saved_path in saved_paths:
            print(f"[INFO] ì €ì¥ëœ S3 ê²½ë¡œ: {saved_path}")
        print(f"[INFO] ì´ ì €ì¥ëœ íŒŒì¼(íŒŒí‹°ì…˜) ê°¯ìˆ˜: {file_count}ê±´")
        
        # staging í´ë”ì— ë°ì´í„° ì‚­ì œ
        # remove_staging_data(spark, s3_bucket, base_s3_prefix, is_incremental)

    except Exception as e:
        print(f"[ERROR] save_to_analytic ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
    
job.commit()
